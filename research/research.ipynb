{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8989726,"sourceType":"datasetVersion","datasetId":5414437},{"sourceId":79450,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":66746,"modelId":91762}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom glob import glob\nimport scipy.io\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T14:20:55.510942Z","iopub.execute_input":"2024-07-19T14:20:55.511977Z","iopub.status.idle":"2024-07-19T14:20:55.521097Z","shell.execute_reply.started":"2024-07-19T14:20:55.511937Z","shell.execute_reply":"2024-07-19T14:20:55.520098Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_unet(input_shape, num_classes):\n    inputs = Input(input_shape)\n    \n    # Load MobileNetV2 as the encoder\n    mobile_net = MobileNetV2(include_top=False, weights='imagenet', input_tensor=inputs)\n    \n    # Define skip connections from MobileNetV2\n    skip_connections = [\n        mobile_net.get_layer(\"block_1_expand_relu\").output,  # 64 filters\n        mobile_net.get_layer(\"block_3_expand_relu\").output,  # 96 filters\n        mobile_net.get_layer(\"block_6_expand_relu\").output,  # 144 filters\n        mobile_net.get_layer(\"block_13_expand_relu\").output  # 384 filters\n    ]\n\n    # Bottom of the U-Net\n    bottom = mobile_net.get_layer(\"block_16_project\").output  # 1280 filters\n\n    # Decoder\n    d1 = decoder_block(bottom, skip_connections[3], 512)\n    d2 = decoder_block(d1, skip_connections[2], 256)\n    d3 = decoder_block(d2, skip_connections[1], 128)\n    d4 = decoder_block(d3, skip_connections[0], 64)\n\n    # Adjust the output layer to ensure the final output shape matches the input shape\n    x = Conv2DTranspose(32, (2, 2), strides=2, padding=\"same\")(d4)\n    x = conv_block(x, 32)\n\n    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x)\n\n    model = Model(inputs, outputs, name=\"U-Net_MobileNetV2\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:27:44.526072Z","iopub.execute_input":"2024-07-19T14:27:44.526875Z","iopub.status.idle":"2024-07-19T14:27:44.538373Z","shell.execute_reply.started":"2024-07-19T14:27:44.526843Z","shell.execute_reply":"2024-07-19T14:27:44.537414Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"global IMG_H\nglobal IMG_W\nglobal NUM_CLASSES\nglobal CLASSES\nglobal COLORMAP","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:37.920875Z","iopub.execute_input":"2024-07-19T14:06:37.921580Z","iopub.status.idle":"2024-07-19T14:06:37.925617Z","shell.execute_reply.started":"2024-07-19T14:06:37.921548Z","shell.execute_reply":"2024-07-19T14:06:37.924700Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\"\"\" Load and split the dataset \"\"\"\ndef load_dataset(path, split=0.2):\n    train_x = sorted(glob(os.path.join(path, \"Training\", \"Images\", \"*\")))[:10000]\n    train_y = sorted(glob(os.path.join(path, \"Training\", \"Categories\", \"*\")))[:10000]\n\n    split_size = int(split * len(train_x))\n\n    train_x, valid_x = train_test_split(train_x, test_size=split_size, random_state=42)\n    train_y, valid_y = train_test_split(train_y, test_size=split_size, random_state=42)\n\n    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:38.873038Z","iopub.execute_input":"2024-07-19T14:06:38.873904Z","iopub.status.idle":"2024-07-19T14:06:38.883969Z","shell.execute_reply.started":"2024-07-19T14:06:38.873869Z","shell.execute_reply":"2024-07-19T14:06:38.882850Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_colormap(path):\n    mat_path = os.path.join(path, \"human_colormap.mat\")\n    colormap = scipy.io.loadmat(mat_path)[\"colormap\"]\n    colormap = colormap * 256\n    colormap = colormap.astype(np.uint8)\n    colormap = [[c[2], c[1], c[0]] for c in colormap]\n\n    classes = [\n        \"Background\",\n        \"Hat\",\n        \"Hair\",\n        \"Glove\",\n        \"Sunglasses\",\n        \"UpperClothes\",\n        \"Dress\",\n        \"Coat\",\n        \"Socks\",\n        \"Pants\",\n        \"Torso-skin\",\n        \"Scarf\",\n        \"Skirt\",\n        \"Face\",\n        \"Left-arm\",\n        \"Right-arm\",\n        \"Left-leg\",\n        \"Right-leg\",\n        \"Left-shoe\",\n        \"Right-shoe\"\n    ]\n\n    return classes, colormap","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:39.372191Z","iopub.execute_input":"2024-07-19T14:06:39.373174Z","iopub.status.idle":"2024-07-19T14:06:39.380130Z","shell.execute_reply.started":"2024-07-19T14:06:39.373139Z","shell.execute_reply":"2024-07-19T14:06:39.379193Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def read_image_mask(x, y):\n    \"\"\" Reading \"\"\"\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    y = cv2.imread(y, cv2.IMREAD_COLOR)\n    assert x.shape == y.shape\n\n    \"\"\" Resizing \"\"\"\n    x = cv2.resize(x, (IMG_W, IMG_H))\n    y = cv2.resize(y, (IMG_W, IMG_H))\n\n    \"\"\" Image processing \"\"\"\n    x = x / 255.0\n    x = x.astype(np.float32)\n\n    \"\"\" Mask processing \"\"\"\n    output = []\n    for color in COLORMAP:\n        cmap = np.all(np.equal(y, color), axis=-1)\n        output.append(cmap)\n    output = np.stack(output, axis=-1)\n    output = output.astype(np.uint8)\n\n    return x, output","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:39.840538Z","iopub.execute_input":"2024-07-19T14:06:39.840930Z","iopub.status.idle":"2024-07-19T14:06:39.848325Z","shell.execute_reply.started":"2024-07-19T14:06:39.840899Z","shell.execute_reply":"2024-07-19T14:06:39.847418Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess(x, y):\n    def f(x, y):\n        x = x.decode()\n        y = y.decode()\n        image, mask = read_image_mask(x, y)\n        return image, mask\n\n    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.uint8])\n    image.set_shape([IMG_H, IMG_W, 3])\n    mask.set_shape([IMG_H, IMG_W, NUM_CLASSES])\n\n    return image, mask\n\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.shuffle(buffer_size=5000)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(2)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:40.612877Z","iopub.execute_input":"2024-07-19T14:06:40.613254Z","iopub.status.idle":"2024-07-19T14:06:40.621312Z","shell.execute_reply.started":"2024-07-19T14:06:40.613228Z","shell.execute_reply":"2024-07-19T14:06:40.620229Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\n\ncreate_dir(\"/kaggle/working/files\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:41.594757Z","iopub.execute_input":"2024-07-19T14:06:41.595145Z","iopub.status.idle":"2024-07-19T14:06:41.600282Z","shell.execute_reply.started":"2024-07-19T14:06:41.595117Z","shell.execute_reply":"2024-07-19T14:06:41.599345Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"IMG_H = 320\nIMG_W = 416\nNUM_CLASSES = 20\ninput_shape = (IMG_H, IMG_W, 3)\n\nbatch_size = 16\nlr = 1e-4\nnum_epochs = 100","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:06:41.957589Z","iopub.execute_input":"2024-07-19T14:06:41.958067Z","iopub.status.idle":"2024-07-19T14:06:41.963054Z","shell.execute_reply.started":"2024-07-19T14:06:41.958035Z","shell.execute_reply":"2024-07-19T14:06:41.962086Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/instance-image-segmentation/instance-level_human_parsing/instance-level_human_parsing\"\n\nmodel_path = os.path.join(\"/kaggle/working/files\", \"model.keras\")\ncsv_path = os.path.join(\"/kaggle/working/files\", \"data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:23:19.047884Z","iopub.execute_input":"2024-07-19T14:23:19.048628Z","iopub.status.idle":"2024-07-19T14:23:19.053164Z","shell.execute_reply.started":"2024-07-19T14:23:19.048595Z","shell.execute_reply":"2024-07-19T14:23:19.052206Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\nprint(f\"Train: {len(train_x)}/{len(train_y)} - Valid: {len(valid_x)}/{len(valid_y)} - Test: {len(test_x)}/{len(test_x)}\")\nprint(\"\")\n\nCLASSES, COLORMAP = get_colormap(dataset_path)\n\ntrain_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:23:27.090685Z","iopub.execute_input":"2024-07-19T14:23:27.091078Z","iopub.status.idle":"2024-07-19T14:23:27.422323Z","shell.execute_reply.started":"2024-07-19T14:23:27.091047Z","shell.execute_reply":"2024-07-19T14:23:27.421430Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Train: 6000/6000 - Valid: 2000/2000 - Test: 2000/2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = build_unet(input_shape, NUM_CLASSES)\n# model.load_weights(model_path)\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=tf.keras.optimizers.Adam(lr),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:27:54.015701Z","iopub.execute_input":"2024-07-19T14:27:54.016539Z","iopub.status.idle":"2024-07-19T14:27:54.981520Z","shell.execute_reply.started":"2024-07-19T14:27:54.016509Z","shell.execute_reply":"2024-07-19T14:27:54.980722Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_435/1812566688.py:22: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  mobile_net = MobileNetV2(include_top=False, weights='imagenet', input_tensor=inputs)\n","output_type":"stream"}]},{"cell_type":"code","source":"callbacks = [\n        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n        CSVLogger(csv_path, append=True),\n        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n    ]\n\nmodel.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=num_epochs,\n    callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:28:20.767494Z","iopub.execute_input":"2024-07-19T14:28:20.767879Z","iopub.status.idle":"2024-07-19T20:10:35.473070Z","shell.execute_reply.started":"2024-07-19T14:28:20.767850Z","shell.execute_reply":"2024-07-19T20:10:35.471814Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2024-07-19 14:29:30.779401: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[128,272,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,272,80,104]{3,2,1,0}, f32[16,128,80,104]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:31.372194: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.592914342s\nTrying algorithm eng0{} for conv (f32[128,272,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,272,80,104]{3,2,1,0}, f32[16,128,80,104]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:32.932836: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[128,272,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,272,80,104]{3,2,1,0}, f32[16,128,80,104]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:33.525180: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.592547102s\nTrying algorithm eng0{} for conv (f32[128,272,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,272,80,104]{3,2,1,0}, f32[16,128,80,104]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:36.552989: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,160,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,160,160,208]{3,2,1,0}, f32[16,64,160,208]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:37.439378: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.886589237s\nTrying algorithm eng0{} for conv (f32[64,160,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,160,160,208]{3,2,1,0}, f32[16,64,160,208]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:39.380305: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,160,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,160,160,208]{3,2,1,0}, f32[16,64,160,208]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n2024-07-19 14:29:40.266536: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.886332219s\nTrying algorithm eng0{} for conv (f32[64,160,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,160,160,208]{3,2,1,0}, f32[16,64,160,208]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0} is taking a while...\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4442 - loss: 2.0942\nEpoch 1: val_loss improved from inf to 1.35768, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 2s/step - accuracy: 0.4447 - loss: 2.0931 - val_accuracy: 0.6786 - val_loss: 1.3577 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7539 - loss: 1.0384\nEpoch 2: val_loss improved from 1.35768 to 1.10712, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 2s/step - accuracy: 0.7539 - loss: 1.0382 - val_accuracy: 0.7103 - val_loss: 1.1071 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7984 - loss: 0.7745\nEpoch 3: val_loss improved from 1.10712 to 0.84696, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 2s/step - accuracy: 0.7984 - loss: 0.7744 - val_accuracy: 0.7688 - val_loss: 0.8470 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8245 - loss: 0.6282\nEpoch 4: val_loss improved from 0.84696 to 0.66938, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 2s/step - accuracy: 0.8245 - loss: 0.6282 - val_accuracy: 0.7961 - val_loss: 0.6694 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8462 - loss: 0.5291\nEpoch 5: val_loss improved from 0.66938 to 0.63249, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 2s/step - accuracy: 0.8462 - loss: 0.5291 - val_accuracy: 0.8074 - val_loss: 0.6325 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8643 - loss: 0.4487\nEpoch 6: val_loss did not improve from 0.63249\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 2s/step - accuracy: 0.8643 - loss: 0.4487 - val_accuracy: 0.7934 - val_loss: 0.6434 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8741 - loss: 0.3979\nEpoch 7: val_loss improved from 0.63249 to 0.61315, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 2s/step - accuracy: 0.8741 - loss: 0.3979 - val_accuracy: 0.8085 - val_loss: 0.6131 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8834 - loss: 0.3584\nEpoch 8: val_loss improved from 0.61315 to 0.59256, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 2s/step - accuracy: 0.8834 - loss: 0.3584 - val_accuracy: 0.8152 - val_loss: 0.5926 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8925 - loss: 0.3250\nEpoch 9: val_loss improved from 0.59256 to 0.58001, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 2s/step - accuracy: 0.8925 - loss: 0.3250 - val_accuracy: 0.8126 - val_loss: 0.5800 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8984 - loss: 0.3052\nEpoch 10: val_loss did not improve from 0.58001\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 2s/step - accuracy: 0.8984 - loss: 0.3052 - val_accuracy: 0.8017 - val_loss: 0.6332 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9042 - loss: 0.2869\nEpoch 11: val_loss improved from 0.58001 to 0.55380, saving model to /kaggle/working/files/model.keras\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 2s/step - accuracy: 0.9042 - loss: 0.2869 - val_accuracy: 0.8234 - val_loss: 0.5538 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9089 - loss: 0.2720\nEpoch 12: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 2s/step - accuracy: 0.9089 - loss: 0.2720 - val_accuracy: 0.8219 - val_loss: 0.5754 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9118 - loss: 0.2649\nEpoch 13: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 2s/step - accuracy: 0.9118 - loss: 0.2649 - val_accuracy: 0.8260 - val_loss: 0.5704 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9181 - loss: 0.2428\nEpoch 14: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 2s/step - accuracy: 0.9181 - loss: 0.2428 - val_accuracy: 0.8283 - val_loss: 0.6063 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9209 - loss: 0.2346\nEpoch 15: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 2s/step - accuracy: 0.9209 - loss: 0.2346 - val_accuracy: 0.8252 - val_loss: 0.6061 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9257 - loss: 0.2206\nEpoch 16: val_loss did not improve from 0.55380\n\nEpoch 16: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 2s/step - accuracy: 0.9257 - loss: 0.2206 - val_accuracy: 0.8278 - val_loss: 0.5970 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9325 - loss: 0.2003\nEpoch 17: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 2s/step - accuracy: 0.9325 - loss: 0.2003 - val_accuracy: 0.8344 - val_loss: 0.5711 - learning_rate: 1.0000e-05\nEpoch 18/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9363 - loss: 0.1879\nEpoch 18: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 2s/step - accuracy: 0.9363 - loss: 0.1879 - val_accuracy: 0.8357 - val_loss: 0.5711 - learning_rate: 1.0000e-05\nEpoch 19/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9373 - loss: 0.1853\nEpoch 19: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 2s/step - accuracy: 0.9374 - loss: 0.1853 - val_accuracy: 0.8358 - val_loss: 0.5729 - learning_rate: 1.0000e-05\nEpoch 20/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9385 - loss: 0.1834\nEpoch 20: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 2s/step - accuracy: 0.9385 - loss: 0.1834 - val_accuracy: 0.8355 - val_loss: 0.5732 - learning_rate: 1.0000e-05\nEpoch 21/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9394 - loss: 0.1797\nEpoch 21: val_loss did not improve from 0.55380\n\nEpoch 21: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 2s/step - accuracy: 0.9394 - loss: 0.1797 - val_accuracy: 0.8361 - val_loss: 0.5775 - learning_rate: 1.0000e-05\nEpoch 22/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9403 - loss: 0.1757\nEpoch 22: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 2s/step - accuracy: 0.9403 - loss: 0.1757 - val_accuracy: 0.8361 - val_loss: 0.5779 - learning_rate: 1.0000e-06\nEpoch 23/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9406 - loss: 0.1758\nEpoch 23: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 2s/step - accuracy: 0.9406 - loss: 0.1758 - val_accuracy: 0.8362 - val_loss: 0.5781 - learning_rate: 1.0000e-06\nEpoch 24/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9410 - loss: 0.1745\nEpoch 24: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 2s/step - accuracy: 0.9410 - loss: 0.1745 - val_accuracy: 0.8364 - val_loss: 0.5794 - learning_rate: 1.0000e-06\nEpoch 25/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9403 - loss: 0.1775\nEpoch 25: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 2s/step - accuracy: 0.9403 - loss: 0.1775 - val_accuracy: 0.8362 - val_loss: 0.5796 - learning_rate: 1.0000e-06\nEpoch 26/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9411 - loss: 0.1746\nEpoch 26: val_loss did not improve from 0.55380\n\nEpoch 26: ReduceLROnPlateau reducing learning rate to 1e-07.\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 2s/step - accuracy: 0.9411 - loss: 0.1746 - val_accuracy: 0.8363 - val_loss: 0.5792 - learning_rate: 1.0000e-06\nEpoch 27/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9415 - loss: 0.1729\nEpoch 27: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 2s/step - accuracy: 0.9415 - loss: 0.1729 - val_accuracy: 0.8363 - val_loss: 0.5798 - learning_rate: 1.0000e-07\nEpoch 28/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9412 - loss: 0.1751\nEpoch 28: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 2s/step - accuracy: 0.9412 - loss: 0.1751 - val_accuracy: 0.8362 - val_loss: 0.5798 - learning_rate: 1.0000e-07\nEpoch 29/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9413 - loss: 0.1731\nEpoch 29: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 2s/step - accuracy: 0.9413 - loss: 0.1731 - val_accuracy: 0.8362 - val_loss: 0.5799 - learning_rate: 1.0000e-07\nEpoch 30/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9407 - loss: 0.1752\nEpoch 30: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 2s/step - accuracy: 0.9407 - loss: 0.1752 - val_accuracy: 0.8362 - val_loss: 0.5799 - learning_rate: 1.0000e-07\nEpoch 31/100\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9412 - loss: 0.1740\nEpoch 31: val_loss did not improve from 0.55380\n\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 2s/step - accuracy: 0.9412 - loss: 0.1740 - val_accuracy: 0.8362 - val_loss: 0.5801 - learning_rate: 1.0000e-07\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b91e083fa00>"},"metadata":{}}]},{"cell_type":"code","source":"tf.saved_model.save(\n  model, '/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:15:12.513960Z","iopub.execute_input":"2024-07-19T20:15:12.514412Z","iopub.status.idle":"2024-07-19T20:15:25.235966Z","shell.execute_reply.started":"2024-07-19T20:15:12.514361Z","shell.execute_reply":"2024-07-19T20:15:25.234826Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Convert the model\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:17:34.462637Z","iopub.execute_input":"2024-07-19T20:17:34.463099Z","iopub.status.idle":"2024-07-19T20:17:34.468607Z","shell.execute_reply.started":"2024-07-19T20:17:34.463065Z","shell.execute_reply":"2024-07-19T20:17:34.467338Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def grayscale_to_rgb(mask, classes, colormap):\n    h, w, _ = mask.shape\n    mask = mask.astype(np.int32)\n    output = []\n\n    for i, pixel in enumerate(mask.flatten()):\n        output.append(colormap[pixel])\n\n    output = np.reshape(output, (h, w, 3))\n    return output\n\ndef save_results(image, mask, pred, save_image_path):\n    h, w, _ = image.shape\n    line = np.ones((h, 10, 3)) * 255\n\n    pred = np.expand_dims(pred, axis=-1)\n    pred = grayscale_to_rgb(pred, CLASSES, COLORMAP)\n\n    cat_images = np.concatenate([image, line, mask, line, pred], axis=1)\n    cv2.imwrite(save_image_path, cat_images)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:17:35.359897Z","iopub.execute_input":"2024-07-19T20:17:35.360919Z","iopub.status.idle":"2024-07-19T20:17:35.369402Z","shell.execute_reply.started":"2024-07-19T20:17:35.360884Z","shell.execute_reply":"2024-07-19T20:17:35.368400Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model_path = os.path.join(\"/kaggle/input/instance-degmentation/tensorflow2/default/1\", \"model.keras\")\nmodel = tf.keras.models.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:07:08.281874Z","iopub.execute_input":"2024-07-19T14:07:08.282599Z","iopub.status.idle":"2024-07-19T14:07:43.481534Z","shell.execute_reply.started":"2024-07-19T14:07:08.282569Z","shell.execute_reply":"2024-07-19T14:07:43.480641Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('/kaggle/working/model.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SCORE = []\nfor x, y in zip(test_x, test_y):\n    name = x.split(\"/\")[-1].split(\".\")[0]\n\n    image = cv2.imread(x, cv2.IMREAD_COLOR)\n    image = cv2.resize(image, (IMG_W, IMG_H))\n    image_x = image\n    image = image/255.0\n    image = np.expand_dims(image, axis=0)\n\n    mask = cv2.imread(y, cv2.IMREAD_COLOR)\n    mask = cv2.resize(mask, (IMG_W, IMG_H))\n    mask_x = mask\n    onehot_mask = []\n    for color in COLORMAP:\n        cmap = np.all(np.equal(mask, color), axis=-1)\n        onehot_mask.append(cmap)\n    onehot_mask = np.stack(onehot_mask, axis=-1)\n    onehot_mask = np.argmax(onehot_mask, axis=-1)\n    onehot_mask = onehot_mask.astype(np.int32)\n\n    pred = model.predict(image, verbose=0)[0]\n    pred = np.argmax(pred, axis=-1)\n    pred = pred.astype(np.float32)\n    \n#     save_image_path = f\"/kaggle/working/files/{name}.png\"\n#     save_results(image_x, mask_x, pred, save_image_path)\n    \n\n    onehot_mask = onehot_mask.flatten()\n    pred = pred.flatten()\n\n    labels = [i for i in range(NUM_CLASSES)]\n\n    f1_value = f1_score(onehot_mask, pred, labels=labels, average=None, zero_division=0)\n    jac_value = jaccard_score(onehot_mask, pred, labels=labels, average=None, zero_division=0)\n\n    SCORE.append([f1_value, jac_value])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:19:53.645266Z","iopub.execute_input":"2024-07-19T20:19:53.646000Z","iopub.status.idle":"2024-07-19T20:29:24.329745Z","shell.execute_reply.started":"2024-07-19T20:19:53.645963Z","shell.execute_reply":"2024-07-19T20:29:24.328667Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"score = np.array(SCORE)\nscore = np.mean(score, axis=0)\n\nf = open(\"/kaggle/working/files/score.csv\", \"w\")\nf.write(\"Class,F1,Jaccard\\n\")\n\nl = [\"Class\", \"F1\", \"Jaccard\"]\nprint(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s}\")\nprint(\"-\"*35)\n\nfor i in range(score.shape[1]):\n    class_name = CLASSES[i]\n    f1 = score[0, i]\n    jac = score[1, i]\n    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n    print(dstr)\n    f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f}\\n\")\n\nprint(\"-\"*35)\nclass_mean = np.mean(score, axis=-1)\nclass_name = \"Mean\"\nf1 = class_mean[0]\njac = class_mean[1]\ndstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\nprint(dstr)\nf.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f}\\n\")\n\nf.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:29:24.331825Z","iopub.execute_input":"2024-07-19T20:29:24.332132Z","iopub.status.idle":"2024-07-19T20:29:24.350091Z","shell.execute_reply.started":"2024-07-19T20:29:24.332101Z","shell.execute_reply":"2024-07-19T20:29:24.349075Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Class           F1         Jaccard   \n-----------------------------------\nBackground     : 0.91726 - 0.85489\nHat            : 0.17345 - 0.14142\nHair           : 0.70944 - 0.60189\nGlove          : 0.00000 - 0.00000\nSunglasses     : 0.00000 - 0.00000\nUpperClothes   : 0.54907 - 0.44535\nDress          : 0.04839 - 0.03812\nCoat           : 0.29541 - 0.24208\nSocks          : 0.00000 - 0.00000\nPants          : 0.40370 - 0.33231\nTorso-skin     : 0.58930 - 0.46888\nScarf          : 0.00258 - 0.00158\nSkirt          : 0.02384 - 0.01836\nFace           : 0.80208 - 0.70738\nLeft-arm       : 0.35982 - 0.27104\nRight-arm      : 0.40433 - 0.30608\nLeft-leg       : 0.06772 - 0.04761\nRight-leg      : 0.05562 - 0.03821\nLeft-shoe      : 0.05771 - 0.03415\nRight-shoe     : 0.07405 - 0.04564\n-----------------------------------\nMean           : 0.27669 - 0.22975\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}